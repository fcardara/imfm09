{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17a5cc4-78ef-4499-8faa-85d27009fe9e",
   "metadata": {},
   "source": [
    "## Paso 3 – Modelo con ajuste de hiperparámetros (`keras_tuner`)\n",
    "\n",
    "En este paso se aplica **búsqueda automática de hiperparámetros** usando la librería [`Keras Tuner`](https://keras.io/keras_tuner/), con el objetivo de encontrar la mejor combinación de arquitectura y configuración del modelo convolucional extendido definido en el paso 2.\n",
    "\n",
    "### Características del modelo:\n",
    "\n",
    "- Se parte del modelo aumentado (más capas convolucionales que el base) y se parametrizan:\n",
    "  - Número de capas convolucionales (`num_conv_layers`)\n",
    "  - Número de filtros (`filters`) y tamaño del kernel (`kernel_size`)\n",
    "  - Función de activación (`relu` o `tanh`)\n",
    "  - Inicialización de pesos (`he_uniform`, `glorot_uniform`)\n",
    "  - Regularización L2 (`kernel_regularizer`)\n",
    "  - Inclusión opcional de `Dropout` con tasa ajustable (`dropout_rate`)\n",
    "  - Número de neuronas en la capa densa (`dense_units`)\n",
    "\n",
    "- Se emplea la técnica de `RandomSearch` para explorar combinaciones posibles de hiperparámetros en un espacio de búsqueda definido.\n",
    "\n",
    "- El modelo se compila con el optimizador `Adam` y se entrena usando `EarlyStopping` para evitar sobreentrenamiento.\n",
    "\n",
    "### Proceso de búsqueda:\n",
    "\n",
    "1. Se definen los espacios de búsqueda de cada hiperparámetro.\n",
    "2. Se ejecutan múltiples configuraciones (`max_trials=10`).\n",
    "3. Se selecciona el modelo con mayor `val_accuracy` como el **mejor modelo encontrado**.\n",
    "4. Se evalúa este modelo óptimo sobre el conjunto de test para comparar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b659f10-fd7d-48ea-ab9d-9ae120a496aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import keras_tuner\n",
    "except ImportError:\n",
    "    print(\"keras-tuner no encontrado. Instalando...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install keras-tuner\n",
    "    import keras_tuner\n",
    "    print(\"keras-tuner instalado correctamente.\")\n",
    "\n",
    "import shutil\n",
    "import os    \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from utils.dataloader import load_data_npy, DataGenerator\n",
    "from utils.model_utils import save_model_and_history, save_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521e220a-2a2d-44e3-9755-cd40142ca08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10220, 150, 150, 3), Validation: (2555, 150, 150, 3), Test: (4259, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- Carga de datos desde .npy ---\n",
    "images_train, categories_train, images_val, categories_val, images_test, categories_test = load_data_npy()\n",
    "\n",
    "# --- Generadores ---\n",
    "train_gen = DataGenerator(images_train, categories_train, batch_size=16)\n",
    "val_gen = DataGenerator(images_val, categories_val, shuffle=False, batch_size=16)\n",
    "test_gen = DataGenerator(images_test, categories_test, shuffle=False, batch_size=16)\n",
    "\n",
    "print(f\"Train: {images_train.shape}, Validation: {images_val.shape}, Test: {images_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08304404-dd16-4f2c-b5c9-f2ee90688f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [02h 59m 06s]\n",
      "val_accuracy: 0.8242661356925964\n",
      "\n",
      "Best val_accuracy So Far: 0.8242661356925964\n",
      "Total elapsed time: 09h 32m 10s\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "2                 |3                 |num_conv_layers\n",
      "64                |32                |filters_0\n",
      "3                 |3                 |kernel_size_0\n",
      "tanh              |tanh              |activation\n",
      "glorot_uniform    |glorot_uniform    |kernel_init\n",
      "7.1292e-05        |0.00010081        |l2\n",
      "64                |128               |filters_1\n",
      "3                 |3                 |kernel_size_1\n",
      "False             |True              |use_dropout\n",
      "256               |64                |dense_units\n",
      "128               |64                |filters_2\n",
      "3                 |3                 |kernel_size_2\n",
      "0.6               |0.4               |dropout_rate\n",
      "\n",
      "Forma de entrada: (150, 150, 3)\n",
      "Número de capas convolucionales: 2\n",
      "  * Capa Conv 0: filtros=64, tamaño kernel=3, activación=tanh\n",
      "     -> Inicializador de kernel: glorot_uniform, Regularización L2: 7.129169641573977e-05\n",
      "  * Capa Conv 1: filtros=64, tamaño kernel=3, activación=tanh\n",
      "     -> Inicializador de kernel: glorot_uniform, Regularización L2: 7.129169641573977e-05\n",
      "Dropout no utilizado\n",
      "Número de neuronas en capa densa: 256\n",
      "Modelo compilado correctamente.\n",
      "Epoch 1/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1138s\u001b[0m 2s/step - accuracy: 0.4501 - loss: 8.8722 - val_accuracy: 0.4630 - val_loss: 1.5444\n",
      "Epoch 2/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m988s\u001b[0m 2s/step - accuracy: 0.5639 - loss: 1.1715 - val_accuracy: 0.4744 - val_loss: 1.4599\n",
      "Epoch 3/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1065s\u001b[0m 2s/step - accuracy: 0.6426 - loss: 0.8964 - val_accuracy: 0.6830 - val_loss: 0.8343\n",
      "Epoch 4/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1067s\u001b[0m 2s/step - accuracy: 0.7156 - loss: 0.7414 - val_accuracy: 0.5190 - val_loss: 1.4501\n",
      "Epoch 5/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1096s\u001b[0m 2s/step - accuracy: 0.7501 - loss: 0.6749 - val_accuracy: 0.5558 - val_loss: 1.3406\n",
      "Epoch 6/15\n",
      "\u001b[1m291/639\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m10:12\u001b[0m 2s/step - accuracy: 0.8055 - loss: 0.5653"
     ]
    }
   ],
   "source": [
    "# --- Definición del modelo para tuning ---\n",
    "def build_model(hp, verbose=True):\n",
    "    model = models.Sequential()\n",
    "    input_shape = (150, 150, 3)\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Forma de entrada: {input_shape}\")\n",
    "\n",
    "    num_conv_layers = hp.Int('num_conv_layers', 2, 3)\n",
    "    if verbose:\n",
    "        print(f\"Número de capas convolucionales: {num_conv_layers}\")\n",
    "\n",
    "    for i in range(num_conv_layers):\n",
    "        filters = hp.Choice(f'filters_{i}', values=[32, 64, 128])\n",
    "        kernel_size = hp.Choice(f'kernel_size_{i}', values=[3, 5])\n",
    "        activation = hp.Choice('activation', ['relu', 'tanh'])\n",
    "        kernel_init = hp.Choice('kernel_init', ['he_uniform', 'glorot_uniform'])\n",
    "        l2_reg = hp.Float('l2', 1e-5, 1e-2, sampling='LOG')\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  * Capa Conv {i}: filtros={filters}, tamaño kernel={kernel_size}, activación={activation}\")\n",
    "            print(f\"     -> Inicializador de kernel: {kernel_init}, Regularización L2: {l2_reg}\")\n",
    "\n",
    "        model.add(layers.Conv2D(filters, (kernel_size, kernel_size),\n",
    "                                activation=activation,\n",
    "                                kernel_initializer=kernel_init,\n",
    "                                kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    use_dropout = hp.Boolean('use_dropout')\n",
    "    if use_dropout:\n",
    "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
    "        if verbose:\n",
    "            print(f\"Dropout activado con tasa: {dropout_rate}\")\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Dropout no utilizado\")\n",
    "\n",
    "    dense_units = hp.Int('dense_units', 64, 256, step=64)\n",
    "    if verbose:\n",
    "        print(f\"Número de neuronas en capa densa: {dense_units}\")\n",
    "\n",
    "    model.add(layers.Dense(units=dense_units, activation='relu'))\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Modelo compilado correctamente.\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "# --- limpieza carpeta tuner_logs/cnn_hyperparam_tuning---\n",
    "shutil.rmtree('tuner_logs/cnn_hyperparam_tuning', ignore_errors=True)\n",
    "\n",
    "# --- Búsqueda de hiperparámetros ---\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_logs',\n",
    "    project_name='cnn_hyperparam_tuning'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "tuner.search(train_gen,\n",
    "             validation_data=val_gen,\n",
    "             epochs=15,\n",
    "             callbacks=[early_stop])\n",
    "\n",
    "# Obtener mejor modelo\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dddf71-5994-41fe-9f99-9b067235b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reentrena el mejor modelo con historial completo\n",
    "history = best_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluación final\n",
    "test_loss, test_acc = best_model.evaluate(test_gen)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}  |  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Ahora sí puedes guardarlo\n",
    "save_model_and_history(best_model, history, model_path='tuned_model')\n",
    "save_test_results('tuned_model', test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916d52a-b7fa-4367-a955-8dbeff319e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
