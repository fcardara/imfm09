{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17a5cc4-78ef-4499-8faa-85d27009fe9e",
   "metadata": {},
   "source": [
    "## Paso 3 – Modelo con ajuste de hiperparámetros (`keras_tuner`)\n",
    "\n",
    "En este notebook se desarrolla el punto 3 del caso práctico:\n",
    "\n",
    "**\"Modelo basado en hyperparameter tuning a través de keras_tuner\"**\n",
    "\n",
    "En este paso se aplica **búsqueda automática de hiperparámetros** usando la librería [`Keras Tuner`](https://keras.io/keras_tuner/), con el objetivo de encontrar la mejor combinación de arquitectura y configuración del modelo convolucional extendido definido en el paso 2.\n",
    "\n",
    "### Características del modelo:\n",
    "\n",
    "- Se parte del modelo aumentado (más capas convolucionales que el base) y se parametrizan:\n",
    "  - Número de capas convolucionales (`num_conv_layers`)\n",
    "  - Número de filtros (`filters`) y tamaño del kernel (`kernel_size`)\n",
    "  - Función de activación (`relu` o `tanh`)\n",
    "  - Inicialización de pesos (`he_uniform`, `glorot_uniform`)\n",
    "  - Regularización L2 (`kernel_regularizer`)\n",
    "  - Inclusión opcional de `Dropout` con tasa ajustable (`dropout_rate`)\n",
    "  - Número de neuronas en la capa densa (`dense_units`)\n",
    "\n",
    "- Se emplea la técnica de `RandomSearch` para explorar combinaciones posibles de hiperparámetros en un espacio de búsqueda definido.\n",
    "\n",
    "- El modelo se compila con el optimizador `Adam` y se entrena usando `EarlyStopping` para evitar sobreentrenamiento.\n",
    "\n",
    "### Proceso de búsqueda:\n",
    "\n",
    "1. Se definen los espacios de búsqueda de cada hiperparámetro.\n",
    "2. Se ejecutan múltiples configuraciones (`max_trials=6`). Originalmente se había configurado con `max_trials=10`, pero debido al tiempo de ejecución elevado en el hardware disponible (más de dos días), se ha reducido a `max_trials=6` para acelerar el proceso manteniendo una diversidad razonable de pruebas.\n",
    "3. Se selecciona el modelo con mayor `val_accuracy` como el **mejor modelo encontrado**.\n",
    "4. Se evalúa este modelo óptimo sobre el conjunto de test para comparar su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b659f10-fd7d-48ea-ab9d-9ae120a496aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 10:43:43.902169: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 10:43:44.170346: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-30 10:43:44.500483: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748601824.761363   14653 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748601824.848189   14653 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748601825.362031   14653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748601825.362343   14653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748601825.362354   14653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748601825.362359   14653 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-30 10:43:45.476194: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import keras_tuner\n",
    "except ImportError:\n",
    "    print(\"keras-tuner no encontrado. Instalando...\")\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install keras-tuner\n",
    "    import keras_tuner\n",
    "    print(\"keras-tuner instalado correctamente.\")\n",
    "\n",
    "import shutil\n",
    "import os    \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from utils.dataloader import load_data_npy, DataGenerator\n",
    "from utils.model_utils import save_model_and_history, save_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521e220a-2a2d-44e3-9755-cd40142ca08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (10220, 150, 150, 3), Validation: (2555, 150, 150, 3), Test: (4259, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "# --- Carga de datos desde .npy ---\n",
    "images_train, categories_train, images_val, categories_val, images_test, categories_test = load_data_npy()\n",
    "\n",
    "# --- Generadores ---\n",
    "train_gen = DataGenerator(images_train, categories_train, batch_size=16)\n",
    "val_gen = DataGenerator(images_val, categories_val, shuffle=False, batch_size=16)\n",
    "test_gen = DataGenerator(images_test, categories_test, shuffle=False, batch_size=16)\n",
    "\n",
    "print(f\"Train: {images_train.shape}, Validation: {images_val.shape}, Test: {images_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08304404-dd16-4f2c-b5c9-f2ee90688f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [02h 18m 11s]\n",
      "val_accuracy: 0.6344422698020935\n",
      "\n",
      "Best val_accuracy So Far: 0.7960861325263977\n",
      "Total elapsed time: 13h 18m 19s\n",
      "Forma de entrada: (150, 150, 3)\n",
      "Número de capas convolucionales: 3\n",
      "  * Capa Conv 0: filtros=32, tamaño kernel=5, activación=tanh\n",
      "     -> Inicializador de kernel: he_uniform, Regularización L2: 4.9730489258714895e-05\n",
      "  * Capa Conv 1: filtros=128, tamaño kernel=3, activación=tanh\n",
      "     -> Inicializador de kernel: he_uniform, Regularización L2: 4.9730489258714895e-05\n",
      "  * Capa Conv 2: filtros=32, tamaño kernel=3, activación=tanh\n",
      "     -> Inicializador de kernel: he_uniform, Regularización L2: 4.9730489258714895e-05\n",
      "Dropout no utilizado\n",
      "Número de neuronas en capa densa: 192\n",
      "Modelo compilado correctamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 34 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# --- Definición del modelo para tuning ---\n",
    "def build_model(hp, verbose=True):\n",
    "    model = models.Sequential()\n",
    "    input_shape = (150, 150, 3)\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Forma de entrada: {input_shape}\")\n",
    "\n",
    "    num_conv_layers = hp.Int('num_conv_layers', 2, 3)\n",
    "    if verbose:\n",
    "        print(f\"Número de capas convolucionales: {num_conv_layers}\")\n",
    "\n",
    "    for i in range(num_conv_layers):\n",
    "        filters = hp.Choice(f'filters_{i}', values=[32, 64, 128])\n",
    "        kernel_size = hp.Choice(f'kernel_size_{i}', values=[3, 5])\n",
    "        activation = hp.Choice('activation', ['relu', 'tanh'])\n",
    "        kernel_init = hp.Choice('kernel_init', ['he_uniform', 'glorot_uniform'])\n",
    "        l2_reg = hp.Float('l2', 1e-5, 1e-2, sampling='LOG')\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  * Capa Conv {i}: filtros={filters}, tamaño kernel={kernel_size}, activación={activation}\")\n",
    "            print(f\"     -> Inicializador de kernel: {kernel_init}, Regularización L2: {l2_reg}\")\n",
    "\n",
    "        model.add(layers.Conv2D(filters, (kernel_size, kernel_size),\n",
    "                                activation=activation,\n",
    "                                kernel_initializer=kernel_init,\n",
    "                                kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    use_dropout = hp.Boolean('use_dropout')\n",
    "    if use_dropout:\n",
    "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
    "        if verbose:\n",
    "            print(f\"Dropout activado con tasa: {dropout_rate}\")\n",
    "        model.add(layers.Dropout(rate=dropout_rate))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"Dropout no utilizado\")\n",
    "\n",
    "    dense_units = hp.Int('dense_units', 64, 256, step=64)\n",
    "    if verbose:\n",
    "        print(f\"Número de neuronas en capa densa: {dense_units}\")\n",
    "\n",
    "    model.add(layers.Dense(units=dense_units, activation='relu'))\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Modelo compilado correctamente.\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "# --- limpieza carpeta tuner_logs/cnn_hyperparam_tuning---\n",
    "shutil.rmtree('tuner_logs/cnn_hyperparam_tuning', ignore_errors=True)\n",
    "\n",
    "# --- Búsqueda de hiperparámetros ---\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=6,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_logs',\n",
    "    project_name='cnn_hyperparam_tuning'\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Ejecutar búsqueda\n",
    "tuner.search(train_gen,\n",
    "             validation_data=val_gen,\n",
    "             epochs=15,\n",
    "             callbacks=[early_stop])\n",
    "\n",
    "# Obtener mejor modelo\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6dddf71-5994-41fe-9f99-9b067235b5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "Epoch 1/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 1s/step - accuracy: 0.9151 - loss: 0.2583 - val_accuracy: 0.7205 - val_loss: 0.9951\n",
      "Epoch 2/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 904ms/step - accuracy: 0.9254 - loss: 0.2437 - val_accuracy: 0.4814 - val_loss: 4.0273\n",
      "Epoch 3/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 826ms/step - accuracy: 0.9290 - loss: 0.2195 - val_accuracy: 0.7288 - val_loss: 1.0452\n",
      "Epoch 4/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 881ms/step - accuracy: 0.9282 - loss: 0.2310 - val_accuracy: 0.7256 - val_loss: 1.1007\n",
      "Epoch 5/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 921ms/step - accuracy: 0.9488 - loss: 0.1861 - val_accuracy: 0.6031 - val_loss: 2.9062\n",
      "Epoch 6/15\n",
      "\u001b[1m639/639\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 956ms/step - accuracy: 0.9510 - loss: 0.1905 - val_accuracy: 0.5644 - val_loss: 2.9617\n",
      "\u001b[1m267/267\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 198ms/step - accuracy: 0.5406 - loss: 3.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 0.5492  |  Test Loss: 2.9407\n",
      "Modelo guardado en: /opt/notebooks/M9/models/tuned_model.h5\n",
      "Historial guardado en: /opt/notebooks/M9/models/tuned_model_history.json\n",
      "Test metrics saved to: models/tuned_model_test_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# Reentrena el mejor modelo con historial completo\n",
    "history = best_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=15,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5)]\n",
    ")\n",
    "\n",
    "# Evaluación final\n",
    "test_loss, test_acc = best_model.evaluate(test_gen)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}  |  Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Ahora sí puedes guardarlo\n",
    "save_model_and_history(best_model, history, model_path='tuned_model')\n",
    "save_test_results('tuned_model', test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916d52a-b7fa-4367-a955-8dbeff319e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
